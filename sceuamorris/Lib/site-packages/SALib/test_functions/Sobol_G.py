import logging
import subprocess
import numpy as np
import os
import time
import sys

#Call the scripts with SCE and
from SCE_python import *
from SCE_functioncall import *


# Non-monotonic Sobol G Function (8 parameters)
# First-order indices:
# x1: 0.7165
# x2: 0.1791
# x3: 0.0237
# x4: 0.0072
# x5-x8: 0.0001
def evaluate(values,bl,bu,x0,problem):
    """Modified Sobol G-function.

    Reverts to original Sobol G-function if delta and alpha are not given.

    .. [1] Saltelli, A., Annoni, P., Azzini, I., Campolongo, F., Ratto, M.,
           Tarantola, S., 2010. Variance based sensitivity analysis of model
           output. Design and estimator for the total sensitivity index.
           Computer Physics Communications 181, 259–270.
           https://doi.org/10.1016/j.cpc.2009.09.018

    Parameters
    ----------
    values : numpy.ndarray
        input variables
    a : numpy.ndarray
        parameter values
    delta : numpy.ndarray
        shift parameters
    alpha : numpy.ndarray
        curvature parameters

    Returns
    -------
    Y : Result of G-function
    """
    start = time.process_time()
    # 3天对应每天的监测数据
    #wq_inst = [0.0635, 1.11325, 0.025, 10.094, 0.061875, 1.07, 0.025, 7.6305, 0.062925, 1.09625, 0.025, 7.448,]
    # 10天对应每天的监测数据
    wq_inst = [0.0635, 1.11325, 0.025, 10.094, 0.061875, 1.07, 0.025, 7.6305, 0.062925, 1.09625, 0.025, 7.448, 0.0666,
               1.07225, 0.025, 7.6725, 0.0708, 1.14475, 0.025, 6.37025, 0.07285, 1.166, 0.038, 6.0095, 0.07435, 1.13475,
               0.03765, 5.90875, 0.0762, 1.13675, 0.03505, 6.3375, 0.074025, 1.12625, 0.033675, 6.57575, 0.0738,
               1.16325, 0.03185, 7.87975]
    #wq_inst=[0.0143,1.11325,0.025,10.094]
    print(wq_inst)

    Y=[]
    maxn = 20000
    kstop = 30
    pcento = 0.01
    peps = 0.01
    iseed = 0
    iniflg = 0
    ngs = 21
    bestx, bestf, BESTX, BESTF, ICALL = sceua(values,wq_inst,Y,x0, bl, bu, maxn, kstop, pcento, peps, ngs, iseed, iniflg,problem, testcase=True,
                                              )

    # if type(values) != np.ndarray:
        # raise TypeError("The argument `values` must be a numpy ndarray")
    #引入SCE-UA的参数律动方法产生参数

    #Y = np.ones([values.shape[0]])
    # 输出次数
   # count = 0

    # for i, row in enumerate(values):
    #for i in n*(param_count+1):
    #    count += 1
   #     print(count)
   #     sim = simulation(row)
    #    like = rmse(wq_inst, sim)
    #    Y[i] = like

    elapsed = (time.process_time() - start)
    print('The calculation of the SCE algorithm took %f seconds' % elapsed)

    ################################################################################
    ##  PLOT PART
    ################################################################################

    '''
    plot the trace of the parametersvalue
    '''
    import matplotlib.pyplot as plt
    fig = plt.figure()
    ax1 = plt.subplot(121)
    ax1.plot(BESTX)
    plt.title('Trace of the different parameters')
    plt.xlabel('Evolution Loop')
    plt.ylabel('Parvalue')

    '''
    Plot the parmaeterspace in 2D with trace of the BESTX
    '''
    # - - - - - - - - - - - - - - - - - - - - - - - - -
    #   make these smaller to increase the resolution
    #dx, dy = 0.05, 0.05
    ##example 5 needs bigger steps!!!
    ##dx, dy = 5., 5.
    # - - - - - - - - - - - - - - - - - - - - - - - - -

    #ax2 = plt.subplot(122)
    #x = np.arange(bl[0], bu[0], dx)
    #y = np.arange(bl[1], bu[1], dy)
    #X, Y = np.meshgrid(x, y)

    #parspace = np.zeros((x.size, y.size))
   # for i in range(x.size):
     #   x1 = x[i]
     #   for j in range(y.size):
      #      x2 = y[j]
     #       parspace[i, j] = EvalObjF(x0.size, np.array([x1, x2]),wq_inst,Y, testcase=True)

    #ax2.pcolor(X, Y, parspace)
    ##plt.colorbar()
    #ax2.plot(BESTX[:, 0], BESTX[:, 1], '*')
   # plt.title('Trace of the BESTX parameter combinations')
    #plt.xlabel('PAR 1')
   # plt.ylabel('PAR 2')

    '''
    Plot the parmaeterspace in 3D - commented out
    '''
    ##from mpl_toolkits.mplot3d import Axes3D
    ##from matplotlib import cm
    ##from matplotlib.ticker import LinearLocator, FormatStrFormatter
    ##fig=plt.figure()
    ##ax = fig.gca(projection='3d')
    ##surf = ax.plot_surface(X, Y, parspace, rstride=8, cstride=8, cmap=cm.jet,linewidth=0, antialiased=False)
    ####cset = ax.contourf(X, Y, parspace, zdir='z', offset=-100)
    ##fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()
    return Y

### 下面是目标函数，纳什效率系数、均方根误差、平均绝对误差
def nashsutcliffe(evaluation, simulation):
    """
    Nash-Sutcliffe model efficinecy

        .. math::

         NSE = 1-\\frac{\\sum_{i=1}^{N}(e_{i}-s_{i})^2}{\\sum_{i=1}^{N}(e_{i}-\\bar{e})^2}

    :evaluation: Observed data to compared with simulation data.
    :type: list

    :simulation: simulation data to compared with evaluation data
    :type: list

    :return: Nash-Sutcliff model efficiency
    :rtype: float

    """
    if len(evaluation) == len(simulation):
        s, e = np.array(simulation), np.array(evaluation)
        # s,e=simulation,evaluation
        mean_observed = np.nanmean(e)
        # compute numerator and denominator
        numerator = np.nansum((e - s) ** 2)
        denominator = np.nansum((e - mean_observed) ** 2)
        # compute coefficient
        return 1 - (numerator / denominator)

    else:
        logging.warning(
            "evaluation and simulation lists does not have the same length."
        )
        return np.nan


def mse(evaluation, simulation):
    """
    Mean Squared Error

        .. math::

         MSE=\\frac{1}{N}\\sum_{i=1}^{N}(e_{i}-s_{i})^2

    :evaluation: Observed data to compared with simulation data.
    :type: list

    :simulation: simulation data to compared with evaluation data
    :type: list

    :return: Mean Squared Error
    :rtype: float
    """

    if len(evaluation) == len(simulation):
        obs, sim = np.array(evaluation), np.array(simulation)
        mse = np.nanmean((obs - sim) ** 2)
        return mse
    else:
        logging.warning(
            "evaluation and simulation lists does not have the same length."
        )
        return np.nan


def rmse(evaluation, simulation):
    """
    Root Mean Squared Error

        .. math::

         RMSE=\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(e_{i}-s_{i})^2}

    :evaluation: Observed data to compared with simulation data.
    :type: list

    :simulation: simulation data to compared with evaluation data
    :type: list

    :return: Root Mean Squared Error
    :rtype: float
    """
    if len(evaluation) == len(simulation) > 0:
        return np.sqrt(mse(evaluation, simulation))
    else:
        logging.warning("evaluation and simulation lists do not have the same length.")
        return np.nan


def mae(evaluation, simulation):
    """
    Mean Absolute Error

        .. math::

         MAE=\\frac{1}{N}\\sum_{i=1}^{N}(\\left |  e_{i}-s_{i} \\right |)

    :evaluation: Observed data to compared with simulation data.
    :type: list

    :simulation: simulation data to compared with evaluation data
    :type: list

    :return: Mean Absolute Error
    :rtype: float
    """
    if len(evaluation) == len(simulation) > 0:
        obs, sim = np.array(evaluation), np.array(simulation)
        mae = np.mean(np.abs(sim - obs))
        return mae
    else:
        logging.warning(
            "evaluation and simulation lists does not have the same length."
        )
        return np.nan


def _partial_first_order_variance(a=None, alpha=None):
    if a is None:
        a = [0, 1, 4.5, 9, 99, 99, 99, 99]
    if alpha is None:
        alpha = np.ones_like(a)
    a = np.array(a)

    return np.divide((alpha ** 2), np.multiply((1 + 2 * alpha), np.square(1 + a)))

def _total_variance(a=None, alpha=None):
    if a is None:
        a = [0, 1, 4.5, 9, 99, 99, 99, 99]
    if alpha is None:
        alpha = np.ones_like(a)

    a = np.array(a)
    return np.add(-1, np.product(1 + _partial_first_order_variance(a, alpha), axis=0))


def sensitivity_index(a, alpha=None):
    a = np.array(a)
    return np.divide(_partial_first_order_variance(a, alpha), _total_variance(a, alpha))


def total_sensitivity_index(a, alpha=None):
    a = np.array(a)

    pv = _partial_first_order_variance(a, alpha)
    tv = _total_variance(a, alpha)
    product_pv = np.product(1 + pv, axis=0)

    return np.divide(pv * np.divide(product_pv, 1 + pv.T), tv)
